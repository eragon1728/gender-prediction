{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca0b00a",
   "metadata": {},
   "source": [
    "# Importing libraries, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e1f5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb3f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec69dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67459378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "import ast\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from scipy.stats import uniform, loguniform, randint, norm, chi2_contingency\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, MaxAbsScaler, PolynomialFeatures, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9f4909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a32502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8401c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59ee0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocessor import TextPreprocessor  # Import the TextPreprocessor class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83b69e",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "We load the raw dataset, entitled \"big.csv\", into a Pandas dataframe 'names'. \n",
    "\n",
    "# EDA \n",
    "\n",
    "We observe that our dataset comprises some 200 million records, consisting of fields 'firstname', 'lastname', and 'gender'. (We shall presently opt to merge firstname and lastname into a single 'name'.)\n",
    "\n",
    "The top entry in each of the three fields happens to be '\\N'. While it makes up a fair proportion of each field, this is most marked in the case of 'gender', where it accounts for an overwhelming 95%. \n",
    "\n",
    "Basic inspection of the dataframe shows that all entries are string-type. An important takeaway is that case-sensitivity is misleading the analysis. (For instance, lastname currently distinguishes between 'Kumar' and 'kumar'; likewise, gender between 'M' and 'm', 'F' and 'f'.) \n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "\n",
    "We uppercase all entries for uniformity. \n",
    "We then drop records where both firstname and lastname are '\\N'; then drop records where gender is anything other than male or female.\n",
    "\n",
    "We concatenate firstname and lastname into a single column 'name' (separated by a space), then strip it of leading and trailing whitespace. \n",
    "We encode gender in binary as 'target'. \n",
    "Retain only name and target as our dataframe.\n",
    "\n",
    "Then observe that name contains some null values, so we drop those records.\n",
    "\n",
    "Observe that some of the most popular names are aliases (specifically, '8888 DATING', 'GAANA USER', 'GUEST \\\\N', and 'GUEST LOGIN'), so we drop those records. \n",
    "\n",
    "We drop \"anti-English\" records, *i.e.,* records where name contains less than two English letters. \n",
    "\n",
    "Next, we drop \"junk\" English records, where name either contains just one distinct letter, or doesn't contain any vowels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d577f",
   "metadata": {},
   "source": [
    "*The dataframe at this stage is entitled 'processed_data'.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94429118",
   "metadata": {},
   "source": [
    "Lastly, we strip the names of all non-alphabetical, non-whitespace characters; then once more strip them of leading and trailing whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31138c7d",
   "metadata": {},
   "source": [
    "**Now, the dataframe is 'new_processed_data'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168fdeae",
   "metadata": {},
   "source": [
    "## Data modification (for consistent labelling):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c330b7f5",
   "metadata": {},
   "source": [
    "### Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764de24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bebad7",
   "metadata": {},
   "source": [
    "### Distinct names, alongside corresponding counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a6593",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# name_counts = df['name'].value_counts()\n",
    "# print(name_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15c825",
   "metadata": {},
   "source": [
    "**4.4 million distinct names.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1708e9ba",
   "metadata": {},
   "source": [
    "### Names with count > 1, alongside their counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193205a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# filtered_names = name_counts[name_counts > 1]\n",
    "# print(filtered_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496d34d",
   "metadata": {},
   "source": [
    "### Number of names with count 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf03367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape[0]-filtered_names.values.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d776d699",
   "metadata": {},
   "source": [
    "### Names with count > 1, alongside splits of their abovementioned true label counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37fb4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter the original dataframe to include only names with count > 1\n",
    "# filtered_df = df[df['name'].isin(filtered_names.index)]\n",
    "\n",
    "# # Group by 'name' and 'gender' and count occurrences\n",
    "# label_counts = filtered_df.groupby(['name', 'target']).size().unstack(fill_value=0)\n",
    "\n",
    "# # Reindex the label_counts DataFrame based on the index of filtered_names\n",
    "# label_counts = label_counts.reindex(index=filtered_names.index)\n",
    "\n",
    "# # Print the count of each label for each name\n",
    "# print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04148e12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Determine the dominant label for each name\n",
    "# label_counts['dominant_label'] = label_counts.idxmax(axis=1)\n",
    "\n",
    "# # Print the dominant label for each name\n",
    "# print(label_counts['dominant_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0caceef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# duplicated_labels = label_counts['dominant_label'].reindex(filtered_names.index).repeat(filtered_names.values)\n",
    "# print(duplicated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be116759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicated_labels_df = duplicated_labels.reset_index()\n",
    "# duplicated_labels_df.columns = ['name', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c068fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_series = name_counts[name_counts == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b7f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_series_df = new_series.reset_index()\n",
    "# new_series_df.columns = ['name', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5aeb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([duplicated_labels_df, new_series_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338d6c4",
   "metadata": {},
   "source": [
    "**The dataframe at this stage is accessible as 'latest_processed_data'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7554d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/root/Abhinav/Gender Prediction Project/latest_processed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4200e51",
   "metadata": {},
   "source": [
    "# Train setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0714fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['name']\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55862b",
   "metadata": {},
   "source": [
    "### Oversampling to balance the (training) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a5bf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample the minority class in the training data\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X.to_frame(), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629613c6",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccfb71e",
   "metadata": {},
   "source": [
    "### Model (pipeline) setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a0bec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with TF-IDF vectorizer and Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dcde3c",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23655ae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline on the oversampled data\n",
    "start_time = time()\n",
    "pipeline.fit(X_resampled['name'], y_resampled)\n",
    "end_time = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e45e337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline time: 251.0804 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pipeline time: {(end_time - start_time):.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709952a4",
   "metadata": {},
   "source": [
    "~4 mins to fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3832c05",
   "metadata": {},
   "source": [
    "### Junk support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "91dd9378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_english_part(input_string):\n",
    "    return ''.join(re.findall(r'[a-zA-Z]+', input_string))\n",
    "\n",
    "def isJunk(input_string):\n",
    "    english_input = extract_english_part(input_string)\n",
    "    return len(set(english_input)) < 3 or not any(char in 'AEIOUaeiou' for char in english_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9d3e7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderPredictor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, pipeline, isJunk):\n",
    "        self.pipeline = pipeline\n",
    "        self.isJunk = isJunk\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for name in X:\n",
    "            if self.isJunk(name):\n",
    "                predictions.append(2)\n",
    "            else:\n",
    "                gender = self.pipeline.predict([name])[0]\n",
    "                predictions.append(gender)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19990d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# Assuming `pipeline` is your trained model and `isJunk` is your checker function\n",
    "gender_predictor = GenderPredictor(pipeline, isJunk)\n",
    "\n",
    "# List of names to predict gender for\n",
    "X = ['Alice', 'Bob', \"sneha jaiswal\", \"abhinav \", 'aaaaa bbbb', '?????']\n",
    "\n",
    "# Predict genders for the names\n",
    "predictions = gender_predictor.predict(X)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f155e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "17b0425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the pipeline to a .pkl file\n",
    "# with open('final_pipeline.pkl', 'wb') as file:\n",
    "#     pickle.dump(pipeline, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
